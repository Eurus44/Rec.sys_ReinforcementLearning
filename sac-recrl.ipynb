{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:30.939879Z",
     "iopub.status.busy": "2023-04-24T13:34:30.938875Z",
     "iopub.status.idle": "2023-04-24T13:34:44.257026Z",
     "shell.execute_reply": "2023-04-24T13:34:44.255576Z",
     "shell.execute_reply.started": "2023-04-24T13:34:30.939829Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install pytorch-ranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:44.261522Z",
     "iopub.status.busy": "2023-04-24T13:34:44.261205Z",
     "iopub.status.idle": "2023-04-24T13:34:48.555577Z",
     "shell.execute_reply": "2023-04-24T13:34:48.554496Z",
     "shell.execute_reply.started": "2023-04-24T13:34:44.261486Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:48.558090Z",
     "iopub.status.busy": "2023-04-24T13:34:48.557407Z",
     "iopub.status.idle": "2023-04-24T13:34:48.567863Z",
     "shell.execute_reply": "2023-04-24T13:34:48.566855Z",
     "shell.execute_reply.started": "2023-04-24T13:34:48.558050Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_ranger import Ranger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as td\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:48.572709Z",
     "iopub.status.busy": "2023-04-24T13:34:48.572165Z",
     "iopub.status.idle": "2023-04-24T13:34:48.585459Z",
     "shell.execute_reply": "2023-04-24T13:34:48.584311Z",
     "shell.execute_reply.started": "2023-04-24T13:34:48.572673Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USE CUDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:48.589112Z",
     "iopub.status.busy": "2023-04-24T13:34:48.588771Z",
     "iopub.status.idle": "2023-04-24T13:34:48.696007Z",
     "shell.execute_reply": "2023-04-24T13:34:48.694646Z",
     "shell.execute_reply.started": "2023-04-24T13:34:48.589082Z"
    }
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# device   = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ULTIL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:48.698655Z",
     "iopub.status.busy": "2023-04-24T13:34:48.698230Z",
     "iopub.status.idle": "2023-04-24T13:34:48.756554Z",
     "shell.execute_reply": "2023-04-24T13:34:48.755502Z",
     "shell.execute_reply.started": "2023-04-24T13:34:48.698615Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch.utils.data as td\n",
    "\n",
    "\n",
    "class EvalDataset(td.Dataset):\n",
    "    def __init__(self, positive_data, item_num, positive_mat, negative_samples=99):\n",
    "        super(EvalDataset, self).__init__()\n",
    "        self.positive_data = np.array(positive_data)\n",
    "        self.item_num = item_num\n",
    "        self.positive_mat = positive_mat\n",
    "        self.negative_samples = negative_samples\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        print(\"Resetting dataset\")\n",
    "        data = self.create_valid_data()\n",
    "        labels = np.zeros(len(self.positive_data) * (1 + self.negative_samples))\n",
    "        labels[::1+self.negative_samples] = 1\n",
    "        self.data = np.concatenate([\n",
    "            np.array(data), \n",
    "            np.array(labels)[:, np.newaxis]], \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    def create_valid_data(self):\n",
    "        valid_data = []\n",
    "        for user, positive in self.positive_data:\n",
    "            valid_data.append([user, positive])\n",
    "            for i in range(self.negative_samples):\n",
    "                negative = np.random.randint(self.item_num)\n",
    "                while (user, negative) in self.positive_mat:\n",
    "                    negative = np.random.randint(self.item_num)\n",
    "                    \n",
    "                valid_data.append([user, negative])\n",
    "        return valid_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, label = self.data[idx]\n",
    "        output = {\n",
    "            \"user\": user,\n",
    "            \"item\": item,\n",
    "            \"label\": np.float32(label),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "\n",
    "#https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n",
    "class OUNoise(object):\n",
    "    def __init__(self, action_dim, mu=0.0, theta=0.15, max_sigma=0.4, min_sigma=0.4, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_dim\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return torch.tensor([action + ou_state]).float()\n",
    "\n",
    "\n",
    "class Prioritized_Buffer(object):\n",
    "    def __init__(self, capacity, prob_alpha=0.6):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []\n",
    "        self.pos        = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "    \n",
    "    def push(self, user, memory, action, reward, next_user, next_memory, done):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((user, memory, action, reward, next_user, next_memory, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (user, memory, action, reward, next_user, next_memory, done)\n",
    "        \n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        \n",
    "        probs  = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total    = len(self.buffer)\n",
    "        weights  = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "\n",
    "        batch       = list(zip(*samples))\n",
    "        user        = np.concatenate(batch[0])\n",
    "        memory      = np.concatenate(batch[1])\n",
    "        action      = batch[2]\n",
    "        reward      = batch[3]\n",
    "        next_user   = np.concatenate(batch[4])\n",
    "        next_memory = np.concatenate(batch[5])\n",
    "        done        = batch[6]\n",
    "\n",
    "        return user, memory, action, reward, next_user, next_memory, done\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def get_beta(idx, beta_start=0.4, beta_steps=100000):\n",
    "    return min(1.0, beta_start + idx * (1.0 - beta_start) / beta_steps)\n",
    "\n",
    "def preprocess_data(data_dir, train_rating):\n",
    "    data = pd.read_csv(os.path.join(data_dir, train_rating), \n",
    "                       sep='\\t', header=None, names=['user', 'item', 'rating'], \n",
    "                       usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.int8})\n",
    "    data = data[data['rating'] > 3][['user', 'item']]\n",
    "    user_num = data['user'].max() + 1\n",
    "    item_num = data['item'].max() + 1\n",
    "\n",
    "    train_data = data.sample(frac=0.8, random_state=16)\n",
    "    test_data = data.drop(train_data.index).values.tolist()\n",
    "    train_data = train_data.values.tolist()\n",
    "\n",
    "    train_mat = defaultdict(int)\n",
    "    test_mat = defaultdict(int)\n",
    "    for user, item in train_data:\n",
    "        train_mat[user, item] = 1.0\n",
    "    for user, item in test_data:\n",
    "        test_mat[user, item] = 1.0\n",
    "    train_matrix = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    dict.update(train_matrix, train_mat)\n",
    "    test_matrix = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    dict.update(test_matrix, test_mat)\n",
    "    \n",
    "    appropriate_users = np.arange(user_num).reshape(-1, 1)[(train_matrix.sum(1) >= 20)]\n",
    "    \n",
    "    return (train_data, train_matrix, test_data, test_matrix, \n",
    "            user_num, item_num, appropriate_users)\n",
    "\n",
    "def to_np(tensor):\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "def hit_metric(recommended, actual):\n",
    "    return int(actual in recommended)\n",
    "\n",
    "def dcg_metric(recommended, actual):\n",
    "    if actual in recommended:\n",
    "        index = recommended.index(actual)\n",
    "        return np.reciprocal(np.log2(index + 2))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:48.758387Z",
     "iopub.status.busy": "2023-04-24T13:34:48.758035Z",
     "iopub.status.idle": "2023-04-24T13:34:48.764938Z",
     "shell.execute_reply": "2023-04-24T13:34:48.763860Z",
     "shell.execute_reply.started": "2023-04-24T13:34:48.758350Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "rating = \"ml-1m.train.rating\"\n",
    "\n",
    "params = {\n",
    "    'batch_size': 512,\n",
    "    'embedding_dim': 8,\n",
    "    'hidden_dim': 16,\n",
    "    'N': 5, # memory size for state_repr\n",
    "    'ou_noise':False,\n",
    "    \n",
    "    'value_lr': 1e-5,\n",
    "    'value_decay': 1e-4,\n",
    "    'policy_lr': 1e-5,\n",
    "    'policy_decay': 1e-6,\n",
    "    'state_repr_lr': 1e-5,\n",
    "    'state_repr_decay': 1e-3,\n",
    "    'log_dir': 'logs/final/',\n",
    "    'gamma': 0.8,\n",
    "    'min_value': -10,\n",
    "    'max_value': 10,\n",
    "    'soft_tau': 1e-3,\n",
    "    \n",
    "    'buffer_size': 1000000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:48.767372Z",
     "iopub.status.busy": "2023-04-24T13:34:48.766667Z",
     "iopub.status.idle": "2023-04-24T13:34:53.803665Z",
     "shell.execute_reply": "2023-04-24T13:34:53.802596Z",
     "shell.execute_reply.started": "2023-04-24T13:34:48.767325Z"
    }
   },
   "outputs": [],
   "source": [
    "# Movielens (1M) data from the https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "    \n",
    "file_path = os.path.join(data_dir, rating)\n",
    "if os.path.exists(file_path):\n",
    "    print(\"Skip loading \" + file_path)\n",
    "else:\n",
    "    with open(file_path, \"wb\") as tf:\n",
    "        print(\"Load \" + file_path)\n",
    "        r = requests.get(\"https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/\" + rating)\n",
    "        tf.write(r.content)\n",
    "(train_data, train_matrix, test_data, test_matrix, \n",
    " user_num, item_num, appropriate_users) = preprocess_data(data_dir, rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft Actor-Critic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:53.805602Z",
     "iopub.status.busy": "2023-04-24T13:34:53.805126Z",
     "iopub.status.idle": "2023-04-24T13:34:53.814114Z",
     "shell.execute_reply": "2023-04-24T13:34:53.812809Z",
     "shell.execute_reply.started": "2023-04-24T13:34:53.805559Z"
    }
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_repr_dim, action_emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_repr_dim + action_emb_dim, hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T18:53:18.585569Z",
     "iopub.status.busy": "2023-04-24T18:53:18.584907Z",
     "iopub.status.idle": "2023-04-24T18:53:18.598731Z",
     "shell.execute_reply": "2023-04-24T18:53:18.597634Z",
     "shell.execute_reply.started": "2023-04-24T18:53:18.585518Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.initialize()\n",
    "        self.to(device)\n",
    "\n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight)\n",
    "        \n",
    "    def evaluate(self, action_emb, epsilon=1e-6):\n",
    "                \n",
    "        mean, log_std = torch.chunk(action_emb.float(), 2, dim=-1)\n",
    "        \n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.sample()\n",
    "\n",
    "        act = torch.tanh(z)\n",
    "        \n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - act.pow(2) + epsilon)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "\n",
    "        return action_emb, log_prob, z, mean, log_std\n",
    "    \n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.layers(state)\n",
    "    \n",
    "    def get_action(self, user, memory, state_repr_sac, \n",
    "                   action_emb,\n",
    "                   items=torch.tensor([i for i in range(item_num)]).to(device),\n",
    "                   return_scores=False\n",
    "                  ):\n",
    "        state = state_repr_sac(user, memory).to(device)\n",
    "        items = items.to(device)\n",
    "        scores = torch.bmm(state_repr_sac.item_embeddings(items).unsqueeze(0), \n",
    "                         action_emb.T.unsqueeze(0)).squeeze(0).to(device)\n",
    "        if return_scores:\n",
    "            return scores, torch.gather(items, 0, scores.argmax(0)).to(device)\n",
    "        else:\n",
    "            return torch.gather(items, 0, scores.argmax(0)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T18:53:26.958128Z",
     "iopub.status.busy": "2023-04-24T18:53:26.957723Z",
     "iopub.status.idle": "2023-04-24T18:53:26.968813Z",
     "shell.execute_reply": "2023-04-24T18:53:26.967727Z",
     "shell.execute_reply.started": "2023-04-24T18:53:26.958095Z"
    }
   },
   "outputs": [],
   "source": [
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:57.099991Z",
     "iopub.status.busy": "2023-04-24T13:34:57.097254Z",
     "iopub.status.idle": "2023-04-24T13:34:57.122673Z",
     "shell.execute_reply": "2023-04-24T13:34:57.121471Z",
     "shell.execute_reply.started": "2023-04-24T13:34:57.099949Z"
    }
   },
   "outputs": [],
   "source": [
    "def soft_q_update(batch_size, \n",
    "           gamma=0.99,\n",
    "           mean_lambda=1e-3,\n",
    "           std_lambda=1e-3,\n",
    "           z_lambda=0.0,\n",
    "           soft_tau=1e-2,\n",
    "          ):\n",
    "    user, memory, action, reward, next_user, next_memory, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    user        = torch.FloatTensor(user).to(device)\n",
    "    memory      = torch.FloatTensor(memory).to(device)\n",
    "    action      = torch.FloatTensor(action).to(device)\n",
    "    reward      = torch.FloatTensor(reward).to(device)\n",
    "    next_user   = torch.FloatTensor(next_user).to(device)\n",
    "    next_memory = torch.FloatTensor(next_memory).to(device)\n",
    "    done        = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "    \n",
    "    state       = state_repr_sac(user, memory)\n",
    "    next_state     = state_repr_sac(next_user, next_memory)\n",
    "\n",
    "    expected_q_value = soft_q_net(state, action)\n",
    "    expected_value   = value_net(state, policy_net(state))\n",
    "    \n",
    "    action_emb = policy_net(state_repr_sac(user, memory))\n",
    "    new_action, log_prob, z, mean, log_std = policy_net.evaluate(action_emb)\n",
    "\n",
    "    target_value = target_value_net(next_state, policy_net(next_state))\n",
    "    next_q_value = reward + (1 - done) * gamma * target_value\n",
    "    q_value_loss = soft_q_criterion(expected_q_value, next_q_value.detach())\n",
    "\n",
    "    expected_new_q_value = soft_q_net(state, new_action)\n",
    "    next_value = expected_new_q_value - log_prob\n",
    "    value_loss = value_criterion(expected_value, next_value.detach())\n",
    "\n",
    "    log_prob_target = expected_new_q_value - expected_value\n",
    "    policy_loss = (log_prob * (log_prob - log_prob_target).detach()).mean()\n",
    "    \n",
    "\n",
    "    mean_loss = mean_lambda * mean.pow(2).mean()\n",
    "    std_loss  = std_lambda  * log_std.pow(2).mean()\n",
    "    z_loss    = z_lambda    * z.pow(2).sum(1).mean()\n",
    "\n",
    "    policy_loss += mean_loss + std_loss + z_loss\n",
    "\n",
    "    soft_q_optimizer.zero_grad()\n",
    "    q_value_loss.backward(retain_graph=True )\n",
    "    soft_q_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward(retain_graph=True )\n",
    "    value_optimizer.step()\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward(retain_graph=True )\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    \n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:57.133369Z",
     "iopub.status.busy": "2023-04-24T13:34:57.130250Z",
     "iopub.status.idle": "2023-04-24T13:34:57.153375Z",
     "shell.execute_reply": "2023-04-24T13:34:57.151139Z",
     "shell.execute_reply.started": "2023-04-24T13:34:57.133322Z"
    }
   },
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, user_item_matrix):\n",
    "        self.matrix = user_item_matrix\n",
    "        self.item_count = item_num\n",
    "        self.memory = np.ones([user_num, params['N']]) * item_num\n",
    "        # memory is initialized as [item_num] * N for each user\n",
    "        # it is padding indexes in state_repr and will result in zero embeddings\n",
    "\n",
    "    def reset(self, user_id):\n",
    "        self.user_id = user_id\n",
    "        self.viewed_items = []\n",
    "        self.related_items = np.argwhere(self.matrix[self.user_id] > 0)[:, 1]\n",
    "        self.num_rele = len(self.related_items)\n",
    "        self.nonrelated_items = np.random.choice(\n",
    "            list(set(range(self.item_count)) - set(self.related_items)), self.num_rele)\n",
    "        self.available_items = np.zeros(self.num_rele * 2)\n",
    "        self.available_items[::2] = self.related_items\n",
    "        self.available_items[1::2] = self.nonrelated_items\n",
    "        \n",
    "        return torch.tensor([self.user_id], device=device), torch.tensor(self.memory[[self.user_id], :], device=device)\n",
    "    \n",
    "    def step(self, action, action_emb=None, buffer=None):\n",
    "        initial_user = self.user_id\n",
    "        initial_memory = self.memory[[initial_user], :]\n",
    "        \n",
    "        reward = float(to_np(action)[0] in self.related_items)\n",
    "        self.viewed_items.append(to_np(action)[0])\n",
    "\n",
    "        if action.is_cuda:\n",
    "            Action = action.cpu()\n",
    "        else:\n",
    "            Action = action\n",
    "\n",
    "        if reward:\n",
    "            if len(Action) == 1:\n",
    "                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [Action]\n",
    "            else:\n",
    "                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [Action[0]]\n",
    "                \n",
    "        if len(self.viewed_items) == len(self.related_items):\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "            \n",
    "        if buffer is not None:\n",
    "            buffer.push(np.array([initial_user]), np.array(initial_memory), to_np(action_emb)[0], \n",
    "                        np.array([reward]), np.array([self.user_id]), self.memory[[self.user_id], :], np.array([reward]))\n",
    "\n",
    "        return torch.tensor([self.user_id], device=device), torch.tensor(self.memory[[self.user_id], :], device=device), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:57.157097Z",
     "iopub.status.busy": "2023-04-24T13:34:57.155487Z",
     "iopub.status.idle": "2023-04-24T13:34:57.168144Z",
     "shell.execute_reply": "2023-04-24T13:34:57.167128Z",
     "shell.execute_reply.started": "2023-04-24T13:34:57.157046Z"
    }
   },
   "outputs": [],
   "source": [
    "class State_Repr_Module(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num+1, embedding_dim, padding_idx=int(item_num))\n",
    "        self.drr_ave = torch.nn.Conv1d(in_channels=params['N'], out_channels=1, kernel_size=1)\n",
    "        \n",
    "        self.initialize()\n",
    "        self.to(device)\n",
    "            \n",
    "    def initialize(self):\n",
    "        nn.init.normal_(self.user_embeddings.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embeddings.weight, std=0.01)\n",
    "        self.item_embeddings.weight.data[-1].zero_()\n",
    "        nn.init.uniform_(self.drr_ave.weight)\n",
    "        self.drr_ave.bias.data.zero_()\n",
    "\n",
    "    def forward(self, user, memory):\n",
    "        user = user.to(device)\n",
    "        memory = memory.to(device)\n",
    "        user_embedding = self.user_embeddings(user.long())\n",
    "        \n",
    "\n",
    "        item_embeddings = self.item_embeddings(memory.long())\n",
    "        drr_ave = self.drr_ave(item_embeddings).squeeze(1)\n",
    "        \n",
    "        return torch.cat((user_embedding, user_embedding * drr_ave, drr_ave), 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:34:57.171772Z",
     "iopub.status.busy": "2023-04-24T13:34:57.171439Z",
     "iopub.status.idle": "2023-04-24T13:35:44.384939Z",
     "shell.execute_reply": "2023-04-24T13:35:44.383855Z",
     "shell.execute_reply.started": "2023-04-24T13:34:57.171715Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_dataset = EvalDataset(\n",
    "    np.array(test_data)[np.array(test_data)[:, 0] == 6039], \n",
    "    item_num, \n",
    "    test_matrix)\n",
    "valid_loader = td.DataLoader(valid_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "full_dataset = EvalDataset(np.array(test_data), item_num, test_matrix)\n",
    "full_loader = td.DataLoader(full_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T18:52:31.206150Z",
     "iopub.status.busy": "2023-04-24T18:52:31.204902Z",
     "iopub.status.idle": "2023-04-24T18:52:31.217889Z",
     "shell.execute_reply": "2023-04-24T18:52:31.216521Z",
     "shell.execute_reply.started": "2023-04-24T18:52:31.206089Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_evaluation(net, state_representation, training_env_memory, loader=valid_loader):\n",
    "    hits = []\n",
    "    dcgs = []\n",
    "    test_env = Env(test_matrix)\n",
    "    test_env.memory = training_env_memory.copy()\n",
    "    user, memory = test_env.reset(int(to_np(next(iter(valid_loader))['user'])[0]))\n",
    "    user.to(device)\n",
    "    memory.to(device)\n",
    "    for batch in loader:\n",
    "        action_emb = net(state_repr_sac(user, memory)).to(device)\n",
    "        scores, action = net.get_action(\n",
    "            batch['user'], \n",
    "            torch.tensor(test_env.memory[to_np(batch['user']).astype(int), :]).to(device), \n",
    "            state_representation, \n",
    "            action_emb,\n",
    "            batch['item'].long(), \n",
    "            return_scores=True\n",
    "        )\n",
    "        user, memory, reward, done = test_env.step(action)\n",
    "\n",
    "        \n",
    "        _, ind = scores[:, 0].topk(10)\n",
    "        predictions = torch.take(batch['item'].to(device), ind).cpu().numpy().tolist()\n",
    "        actual = batch['item'][0].item()\n",
    "        hits.append(hit_metric(predictions, actual))\n",
    "        dcgs.append(dcg_metric(predictions, actual))\n",
    "        \n",
    "    return np.mean(hits), np.mean(dcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:35:44.400532Z",
     "iopub.status.busy": "2023-04-24T13:35:44.400003Z",
     "iopub.status.idle": "2023-04-24T13:35:51.815286Z",
     "shell.execute_reply": "2023-04-24T13:35:51.814150Z",
     "shell.execute_reply.started": "2023-04-24T13:35:44.400485Z"
    }
   },
   "outputs": [],
   "source": [
    "train_env = Env(train_matrix)\n",
    "\n",
    "\n",
    "state_repr_sac = State_Repr_Module(user_num, item_num, params['embedding_dim'] , params['hidden_dim']).to(device)\n",
    "value_net  = ValueNetwork(params['embedding_dim'] * 3, params['embedding_dim'], params['hidden_dim']).to(device)\n",
    "target_value_net = ValueNetwork(params['embedding_dim'] * 3, params['embedding_dim'], params['hidden_dim']).to(device)\n",
    "\n",
    "soft_q_net = SoftQNetwork(params['embedding_dim'] * 3, params['embedding_dim'], params['hidden_dim']).to(device)\n",
    "\n",
    "policy_net = PolicyNetwork(params['embedding_dim'], params['hidden_dim']).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "\n",
    "value_criterion  = nn.MSELoss()\n",
    "soft_q_criterion = nn.MSELoss()\n",
    "\n",
    "value_lr  = 3e-4\n",
    "soft_q_lr = 3e-4\n",
    "policy_lr = 3e-4\n",
    "\n",
    "value_optimizer  = optim.Adam(value_net.parameters(), lr=value_lr)\n",
    "soft_q_optimizer = optim.Adam(soft_q_net.parameters(), lr=soft_q_lr)\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "\n",
    "replay_buffer_size = params['buffer_size']\n",
    "replay_buffer = Prioritized_Buffer(replay_buffer_size)\n",
    "writer = SummaryWriter(log_dir=params['log_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:35:51.818950Z",
     "iopub.status.busy": "2023-04-24T13:35:51.817325Z",
     "iopub.status.idle": "2023-04-24T13:35:51.826635Z",
     "shell.execute_reply": "2023-04-24T13:35:51.825486Z",
     "shell.execute_reply.started": "2023-04-24T13:35:51.818900Z"
    }
   },
   "outputs": [],
   "source": [
    "steps = []\n",
    "rewards     = []\n",
    "batch_size  = 128\n",
    "np.random.seed(16)\n",
    "hits, dcgs = [], []\n",
    "hits_all, dcgs_all = [], []\n",
    "step, best_step = 0, 0\n",
    "step, best_step, best_step_all = 0, 0, 0\n",
    "users = np.random.permutation(appropriate_users)\n",
    "ou_noise = OUNoise(params['embedding_dim'], decay_period=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T13:35:51.829310Z",
     "iopub.status.busy": "2023-04-24T13:35:51.828498Z",
     "iopub.status.idle": "2023-04-24T18:44:58.224016Z",
     "shell.execute_reply": "2023-04-24T18:44:58.222862Z",
     "shell.execute_reply.started": "2023-04-24T13:35:51.829269Z"
    }
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "users = np.random.permutation(appropriate_users)\n",
    "for u in tqdm.tqdm(users):\n",
    "    user, memory = train_env.reset(u)\n",
    "    user = user.to(device)\n",
    "    memory = memory.to(device)\n",
    "    for t in range(int(train_matrix[u].sum())):\n",
    "        action_emb = policy_net(state_repr_sac(user, memory)) \n",
    "        \n",
    "        items = torch.tensor(\n",
    "                    [item for item in train_env.available_items \n",
    "                    if item not in train_env.viewed_items]\n",
    "                ).long().to(device)\n",
    "        action = policy_net.get_action(\n",
    "                user, \n",
    "                torch.tensor(train_env.memory[to_np(user).astype(int), :]).to(device), \n",
    "                state_repr_sac, \n",
    "                action_emb,\n",
    "                torch.tensor(\n",
    "                    [item for item in train_env.available_items \n",
    "                    if item not in train_env.viewed_items]\n",
    "                ).long().to(device)\n",
    "            )\n",
    "        user, memory, reward, done = train_env.step(action, action_emb, buffer=replay_buffer)\n",
    "        user = user.to(device)\n",
    "        memory = memory.to(device)\n",
    "        \n",
    "        if len(replay_buffer) > params['batch_size']:\n",
    "                soft_q_update(batch_size)\n",
    "                \n",
    "        if step % 100 == 0 and step > 0:\n",
    "                hit, dcg = run_evaluation(policy_net, state_repr_sac, train_env.memory)\n",
    "                writer.add_scalar('hit', hit, step)\n",
    "                writer.add_scalar('dcg', dcg, step)\n",
    "                hits.append(hit)\n",
    "                dcgs.append(dcg)\n",
    "                steps.append(step)\n",
    "                if np.mean(np.array([hit, dcg]) - np.array([hits[best_step], dcgs[best_step]])) > 0:\n",
    "                    best_step = step // 100\n",
    "                    torch.save(policy_net.state_dict(), params['log_dir'] + 'policy_net.pth')\n",
    "                    torch.save(value_net.state_dict(), params['log_dir'] + 'value_net.pth')\n",
    "                    torch.save(state_repr_sac.state_dict(), params['log_dir'] + 'state_repr.pth')\n",
    "        if step % 10000 == 0 and step > 0:\n",
    "            hit, dcg = run_evaluation(policy_net, state_repr_sac, train_env.memory, full_loader)\n",
    "            writer.add_scalar('hit_all', hit, step)\n",
    "            writer.add_scalar('dcg_all', dcg, step)\n",
    "            hits_all.append(hit)\n",
    "            dcgs_all.append(dcg)\n",
    "            if np.mean(np.array([hit, dcg]) - np.array([hits_all[best_step_all], dcgs_all[best_step_all]])) > 0:\n",
    "                best_step_all = step // 10000\n",
    "                torch.save(policy_net.state_dict(), params['log_dir'] + 'best_policy_net.pth')\n",
    "                torch.save(value_net.state_dict(), params['log_dir'] + 'best_value_net.pth')\n",
    "                torch.save(state_repr_sac.state_dict(), params['log_dir'] + 'best_state_repr.pth')\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T18:44:58.226609Z",
     "iopub.status.busy": "2023-04-24T18:44:58.225483Z",
     "iopub.status.idle": "2023-04-24T18:44:58.517444Z",
     "shell.execute_reply": "2023-04-24T18:44:58.516402Z",
     "shell.execute_reply.started": "2023-04-24T18:44:58.226563Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(steps, dcgs, label='nDCG')\n",
    "plt.plot(steps, hits, label='Hit rates')\n",
    "\n",
    "\n",
    "plt.xlabel('Step')\n",
    "plt.title('Hit Rates and nDCG in Each Step')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T18:44:58.521102Z",
     "iopub.status.busy": "2023-04-24T18:44:58.518910Z",
     "iopub.status.idle": "2023-04-24T18:44:58.529776Z",
     "shell.execute_reply": "2023-04-24T18:44:58.528733Z",
     "shell.execute_reply.started": "2023-04-24T18:44:58.521054Z"
    }
   },
   "outputs": [],
   "source": [
    "np.max(hits_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T18:44:58.533342Z",
     "iopub.status.busy": "2023-04-24T18:44:58.533022Z",
     "iopub.status.idle": "2023-04-24T18:44:58.543757Z",
     "shell.execute_reply": "2023-04-24T18:44:58.542661Z",
     "shell.execute_reply.started": "2023-04-24T18:44:58.533298Z"
    }
   },
   "outputs": [],
   "source": [
    "np.max(dcgs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T18:52:37.301217Z",
     "iopub.status.busy": "2023-04-24T18:52:37.300504Z",
     "iopub.status.idle": "2023-04-24T18:52:37.311524Z",
     "shell.execute_reply": "2023-04-24T18:52:37.310536Z",
     "shell.execute_reply.started": "2023-04-24T18:52:37.301178Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), params['log_dir'] + 'policy_net_final.pth')\n",
    "torch.save(value_net.state_dict(), params['log_dir'] + 'value_net_final.pth')\n",
    "torch.save(state_repr_sac.state_dict(), params['log_dir'] + 'state_repr_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T18:52:38.254021Z",
     "iopub.status.busy": "2023-04-24T18:52:38.253308Z",
     "iopub.status.idle": "2023-04-24T18:52:38.260734Z",
     "shell.execute_reply": "2023-04-24T18:52:38.259547Z",
     "shell.execute_reply.started": "2023-04-24T18:52:38.253981Z"
    }
   },
   "outputs": [],
   "source": [
    "# we need memory for validation, so it's better to save it and not wait next time \n",
    "with open('logs/memory.pickle', 'wb') as f:\n",
    "    pickle.dump(train_env.memory, f)\n",
    "    \n",
    "with open('logs/memory.pickle', 'rb') as f:\n",
    "    memory = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T18:53:35.270844Z",
     "iopub.status.busy": "2023-04-24T18:53:35.269725Z",
     "iopub.status.idle": "2023-04-24T18:56:44.918208Z",
     "shell.execute_reply": "2023-04-24T18:56:44.916989Z",
     "shell.execute_reply.started": "2023-04-24T18:53:35.270760Z"
    }
   },
   "outputs": [],
   "source": [
    "no_ou_state_repr = State_Repr_Module(user_num, item_num, params['embedding_dim'], params['hidden_dim'])\n",
    "no_ou_policy_net = PolicyNetwork(params['embedding_dim'], params['hidden_dim'])\n",
    "no_ou_state_repr.load_state_dict(torch.load('logs/final/' + 'state_repr_final.pth'))\n",
    "no_ou_policy_net.load_state_dict(torch.load('logs/final/' + 'policy_net_final.pth'))\n",
    "    \n",
    "hit, dcg = run_evaluation(no_ou_policy_net, no_ou_state_repr, memory, full_loader)\n",
    "print('hit rate: ', hit, 'dcg: ', dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T18:56:44.921087Z",
     "iopub.status.busy": "2023-04-24T18:56:44.920400Z",
     "iopub.status.idle": "2023-04-24T18:59:54.711404Z",
     "shell.execute_reply": "2023-04-24T18:59:54.710151Z",
     "shell.execute_reply.started": "2023-04-24T18:56:44.921044Z"
    }
   },
   "outputs": [],
   "source": [
    "no_ou_state_repr = State_Repr_Module(user_num, item_num, params['embedding_dim'], params['hidden_dim'])\n",
    "no_ou_policy_net = PolicyNetwork(params['embedding_dim'], params['hidden_dim'])\n",
    "no_ou_state_repr.load_state_dict(torch.load('logs/final/' + 'best_state_repr.pth'))\n",
    "no_ou_policy_net.load_state_dict(torch.load('logs/final/' + 'best_policy_net.pth'))\n",
    "    \n",
    "hit, dcg = run_evaluation(no_ou_policy_net, no_ou_state_repr, memory, full_loader)\n",
    "print('hit rate: ', hit, 'dcg: ', dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T19:00:11.734311Z",
     "iopub.status.busy": "2023-04-24T19:00:11.733321Z",
     "iopub.status.idle": "2023-04-24T19:00:11.740236Z",
     "shell.execute_reply": "2023-04-24T19:00:11.739035Z",
     "shell.execute_reply.started": "2023-04-24T19:00:11.734254Z"
    }
   },
   "outputs": [],
   "source": [
    "random_user = np.random.randint(user_num)\n",
    "print(random_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T19:07:34.674230Z",
     "iopub.status.busy": "2023-04-24T19:07:34.673504Z",
     "iopub.status.idle": "2023-04-24T19:07:34.709491Z",
     "shell.execute_reply": "2023-04-24T19:07:34.708371Z",
     "shell.execute_reply.started": "2023-04-24T19:07:34.674191Z"
    }
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('/kaggle/input/movielens/movies.dat', sep='::', header=None, engine='python', names=['id', 'name', 'genre'],  encoding='ISO-8859-1')\n",
    "# in the code numeration starts with 0\n",
    "movies[movies['id'].isin(np.argwhere(test_matrix[random_user] > 0)[:, 1] + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T19:21:19.660072Z",
     "iopub.status.busy": "2023-04-24T19:21:19.658860Z",
     "iopub.status.idle": "2023-04-24T19:21:19.679240Z",
     "shell.execute_reply": "2023-04-24T19:21:19.677943Z",
     "shell.execute_reply.started": "2023-04-24T19:21:19.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "model = no_ou_policy_net\n",
    "\n",
    "for model, state_representation in zip([no_ou_policy_net], [no_ou_state_repr]):\n",
    "    example_env = Env(test_matrix)\n",
    "    user, memory = example_env.reset(random_user)\n",
    "\n",
    "    user, memory, reward, _ = example_env.step(torch.tensor([3706]))\n",
    "    user, memory, reward, _ = example_env.step(torch.tensor([1584]))\n",
    "\n",
    "    preds = []\n",
    "    for _ in range(3):\n",
    "        action_emb = model(state_representation(user, memory))\n",
    "        action = model.get_action(\n",
    "            user, \n",
    "            torch.tensor(example_env.memory[to_np(user).astype(int), :]), \n",
    "            state_representation, \n",
    "            action_emb,\n",
    "            torch.tensor(\n",
    "                [item for item in example_env.available_items \n",
    "                if item not in example_env.viewed_items]\n",
    "            ).long()\n",
    "        )\n",
    "        user, memory, reward, _ = example_env.step(action)\n",
    "        preds.append(action)\n",
    "\n",
    "    predictions.append(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T19:21:21.897071Z",
     "iopub.status.busy": "2023-04-24T19:21:21.896117Z",
     "iopub.status.idle": "2023-04-24T19:21:21.904309Z",
     "shell.execute_reply": "2023-04-24T19:21:21.903228Z",
     "shell.execute_reply.started": "2023-04-24T19:21:21.897019Z"
    }
   },
   "outputs": [],
   "source": [
    "print(predictions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
