{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:18.472355Z",
     "iopub.status.busy": "2023-04-24T21:02:18.471897Z",
     "iopub.status.idle": "2023-04-24T21:02:30.201529Z",
     "shell.execute_reply": "2023-04-24T21:02:30.200275Z",
     "shell.execute_reply.started": "2023-04-24T21:02:18.472313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-ranger in /opt/conda/lib/python3.7/site-packages (0.1.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pytorch-ranger) (1.13.0+cpu)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->pytorch-ranger) (4.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-ranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:30.204195Z",
     "iopub.status.busy": "2023-04-24T21:02:30.203831Z",
     "iopub.status.idle": "2023-04-24T21:02:30.211917Z",
     "shell.execute_reply": "2023-04-24T21:02:30.210453Z",
     "shell.execute_reply.started": "2023-04-24T21:02:30.204157Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:30.214367Z",
     "iopub.status.busy": "2023-04-24T21:02:30.213952Z",
     "iopub.status.idle": "2023-04-24T21:02:30.224548Z",
     "shell.execute_reply": "2023-04-24T21:02:30.223394Z",
     "shell.execute_reply.started": "2023-04-24T21:02:30.214327Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_ranger import Ranger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as td\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:30.227704Z",
     "iopub.status.busy": "2023-04-24T21:02:30.226980Z",
     "iopub.status.idle": "2023-04-24T21:02:30.238363Z",
     "shell.execute_reply": "2023-04-24T21:02:30.236990Z",
     "shell.execute_reply.started": "2023-04-24T21:02:30.227646Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USE CUDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:30.240908Z",
     "iopub.status.busy": "2023-04-24T21:02:30.240468Z",
     "iopub.status.idle": "2023-04-24T21:02:30.248891Z",
     "shell.execute_reply": "2023-04-24T21:02:30.247646Z",
     "shell.execute_reply.started": "2023-04-24T21:02:30.240868Z"
    }
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ULTIL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:30.251437Z",
     "iopub.status.busy": "2023-04-24T21:02:30.250865Z",
     "iopub.status.idle": "2023-04-24T21:02:30.294010Z",
     "shell.execute_reply": "2023-04-24T21:02:30.292595Z",
     "shell.execute_reply.started": "2023-04-24T21:02:30.251388Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import torch.utils.data as td\n",
    "\n",
    "\n",
    "class EvalDataset(td.Dataset):\n",
    "    def __init__(self, positive_data, item_num, positive_mat, negative_samples=99):\n",
    "        super(EvalDataset, self).__init__()\n",
    "        self.positive_data = np.array(positive_data)\n",
    "        self.item_num = item_num\n",
    "        self.positive_mat = positive_mat\n",
    "        self.negative_samples = negative_samples\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        print(\"Resetting dataset\")\n",
    "        data = self.create_valid_data()\n",
    "        labels = np.zeros(len(self.positive_data) * (1 + self.negative_samples))\n",
    "        labels[::1+self.negative_samples] = 1\n",
    "        self.data = np.concatenate([\n",
    "            np.array(data), \n",
    "            np.array(labels)[:, np.newaxis]], \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    def create_valid_data(self):\n",
    "        valid_data = []\n",
    "        for user, positive in self.positive_data:\n",
    "            valid_data.append([user, positive])\n",
    "            for i in range(self.negative_samples):\n",
    "                negative = np.random.randint(self.item_num)\n",
    "                while (user, negative) in self.positive_mat:\n",
    "                    negative = np.random.randint(self.item_num)\n",
    "                    \n",
    "                valid_data.append([user, negative])\n",
    "        return valid_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, label = self.data[idx]\n",
    "        output = {\n",
    "            \"user\": user,\n",
    "            \"item\": item,\n",
    "            \"label\": np.float32(label),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "\n",
    "#https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n",
    "class OUNoise(object):\n",
    "    def __init__(self, action_dim, mu=0.0, theta=0.15, max_sigma=0.4, min_sigma=0.4, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_dim\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return torch.tensor([action + ou_state]).float()\n",
    "\n",
    "\n",
    "class Prioritized_Buffer(object):\n",
    "    def __init__(self, capacity, prob_alpha=0.6):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []\n",
    "        self.pos        = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "    \n",
    "    def push(self, user, memory, action, reward, next_user, next_memory, done):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((user, memory, action, reward, next_user, next_memory, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (user, memory, action, reward, next_user, next_memory, done)\n",
    "        \n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        \n",
    "        probs  = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total    = len(self.buffer)\n",
    "        weights  = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "\n",
    "        batch       = list(zip(*samples))\n",
    "        user        = np.concatenate(batch[0])\n",
    "        memory      = np.concatenate(batch[1])\n",
    "        action      = batch[2]\n",
    "        reward      = batch[3]\n",
    "        next_user   = np.concatenate(batch[4])\n",
    "        next_memory = np.concatenate(batch[5])\n",
    "        done        = batch[6]\n",
    "\n",
    "        return user, memory, action, reward, next_user, next_memory, done\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def get_beta(idx, beta_start=0.4, beta_steps=100000):\n",
    "    return min(1.0, beta_start + idx * (1.0 - beta_start) / beta_steps)\n",
    "\n",
    "def preprocess_data(data_dir, train_rating):\n",
    "    data = pd.read_csv(os.path.join(data_dir, train_rating), \n",
    "                       sep='\\t', header=None, names=['user', 'item', 'rating'], \n",
    "                       usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.int8})\n",
    "    data = data[data['rating'] > 3][['user', 'item']]\n",
    "    user_num = data['user'].max() + 1\n",
    "    item_num = data['item'].max() + 1\n",
    "\n",
    "    train_data = data.sample(frac=0.8, random_state=16)\n",
    "    test_data = data.drop(train_data.index).values.tolist()\n",
    "    train_data = train_data.values.tolist()\n",
    "\n",
    "    train_mat = defaultdict(int)\n",
    "    test_mat = defaultdict(int)\n",
    "    for user, item in train_data:\n",
    "        train_mat[user, item] = 1.0\n",
    "    for user, item in test_data:\n",
    "        test_mat[user, item] = 1.0\n",
    "    train_matrix = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    dict.update(train_matrix, train_mat)\n",
    "    test_matrix = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    dict.update(test_matrix, test_mat)\n",
    "    \n",
    "    appropriate_users = np.arange(user_num).reshape(-1, 1)[(train_matrix.sum(1) >= 20)]\n",
    "    \n",
    "    return (train_data, train_matrix, test_data, test_matrix, \n",
    "            user_num, item_num, appropriate_users)\n",
    "\n",
    "def to_np(tensor):\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "def hit_metric(recommended, actual):\n",
    "    return int(actual in recommended)\n",
    "\n",
    "def dcg_metric(recommended, actual):\n",
    "    if actual in recommended:\n",
    "        index = recommended.index(actual)\n",
    "        return np.reciprocal(np.log2(index + 2))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:30.295709Z",
     "iopub.status.busy": "2023-04-24T21:02:30.295360Z",
     "iopub.status.idle": "2023-04-24T21:02:30.311227Z",
     "shell.execute_reply": "2023-04-24T21:02:30.309608Z",
     "shell.execute_reply.started": "2023-04-24T21:02:30.295674Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "rating = \"ml-1m.train.rating\"\n",
    "\n",
    "params = {\n",
    "    'batch_size': 512,\n",
    "    'embedding_dim': 8,\n",
    "    'hidden_dim': 16,\n",
    "    'N': 5, # memory size for state_repr\n",
    "    'ou_noise':False,\n",
    "    \n",
    "    'value_lr': 1e-5,\n",
    "    'value_decay': 1e-4,\n",
    "    'policy_lr': 1e-5,\n",
    "    'policy_decay': 1e-6,\n",
    "    'state_repr_lr': 1e-5,\n",
    "    'state_repr_decay': 1e-3,\n",
    "    'log_dir': 'logs/final/',\n",
    "    'gamma': 0.8,\n",
    "    'min_value': -10,\n",
    "    'max_value': 10,\n",
    "    'soft_tau': 1e-3,\n",
    "    \n",
    "    'buffer_size': 1000000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:30.312965Z",
     "iopub.status.busy": "2023-04-24T21:02:30.312567Z",
     "iopub.status.idle": "2023-04-24T21:02:33.892692Z",
     "shell.execute_reply": "2023-04-24T21:02:33.891253Z",
     "shell.execute_reply.started": "2023-04-24T21:02:30.312929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip loading data/ml-1m.train.rating\n"
     ]
    }
   ],
   "source": [
    "# Movielens (1M) data from the https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "    \n",
    "file_path = os.path.join(data_dir, rating)\n",
    "if os.path.exists(file_path):\n",
    "    print(\"Skip loading \" + file_path)\n",
    "else:\n",
    "    with open(file_path, \"wb\") as tf:\n",
    "        print(\"Load \" + file_path)\n",
    "        r = requests.get(\"https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/\" + rating)\n",
    "        tf.write(r.content)\n",
    "(train_data, train_matrix, test_data, test_matrix, \n",
    " user_num, item_num, appropriate_users) = preprocess_data(data_dir, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:33.894750Z",
     "iopub.status.busy": "2023-04-24T21:02:33.894353Z",
     "iopub.status.idle": "2023-04-24T21:02:33.924065Z",
     "shell.execute_reply": "2023-04-24T21:02:33.922612Z",
     "shell.execute_reply.started": "2023-04-24T21:02:33.894711Z"
    }
   },
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, user_item_matrix):\n",
    "        self.matrix = user_item_matrix\n",
    "        self.item_count = item_num\n",
    "        self.memory = np.ones([user_num, params['N']]) * item_num\n",
    "        # memory is initialized as [item_num] * N for each user\n",
    "        # it is padding indexes in state_repr and will result in zero embeddings\n",
    "\n",
    "    def reset(self, user_id):\n",
    "        self.user_id = user_id\n",
    "        self.viewed_items = []\n",
    "        self.related_items = np.argwhere(self.matrix[self.user_id] > 0)[:, 1]\n",
    "        self.num_rele = len(self.related_items)\n",
    "        self.nonrelated_items = np.random.choice(\n",
    "            list(set(range(self.item_count)) - set(self.related_items)), self.num_rele)\n",
    "        self.available_items = np.zeros(self.num_rele * 2)\n",
    "        self.available_items[::2] = self.related_items\n",
    "        self.available_items[1::2] = self.nonrelated_items\n",
    "        \n",
    "        return torch.tensor([self.user_id], device=device), torch.tensor(self.memory[[self.user_id], :], device=device)\n",
    "    \n",
    "    def step(self, action, action_emb=None, buffer=None):\n",
    "        initial_user = self.user_id\n",
    "        initial_memory = self.memory[[initial_user], :]\n",
    "        \n",
    "        reward = float(to_np(action)[0] in self.related_items)\n",
    "        self.viewed_items.append(to_np(action)[0])\n",
    "\n",
    "        if action.is_cuda:\n",
    "            Action = action.cpu()\n",
    "        else:\n",
    "            Action = action\n",
    "        \n",
    "        if reward:\n",
    "            if len(Action) == 1:\n",
    "                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [Action]\n",
    "            else:\n",
    "                self.memory[self.user_id] = list(self.memory[self.user_id][1:]) + [Action[0]]\n",
    "                \n",
    "        if len(self.viewed_items) == len(self.related_items):\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "            \n",
    "        if buffer is not None:\n",
    "            buffer.push(np.array([initial_user]), np.array(initial_memory), to_np(action_emb)[0], \n",
    "                        np.array([reward]), np.array([self.user_id]), self.memory[[self.user_id], :], np.array([reward]))\n",
    "\n",
    "        return torch.tensor([self.user_id], device=device), torch.tensor(self.memory[[self.user_id], :], device=device), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:33.929104Z",
     "iopub.status.busy": "2023-04-24T21:02:33.928549Z",
     "iopub.status.idle": "2023-04-24T21:02:33.941159Z",
     "shell.execute_reply": "2023-04-24T21:02:33.939721Z",
     "shell.execute_reply.started": "2023-04-24T21:02:33.929065Z"
    }
   },
   "outputs": [],
   "source": [
    "class State_Repr_Module(nn.Module):\n",
    "    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num+1, embedding_dim, padding_idx=int(item_num))\n",
    "        self.drr_ave = torch.nn.Conv1d(in_channels=params['N'], out_channels=1, kernel_size=1)\n",
    "        \n",
    "        self.initialize()\n",
    "        self.to(device)\n",
    "            \n",
    "    def initialize(self):\n",
    "        nn.init.normal_(self.user_embeddings.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embeddings.weight, std=0.01)\n",
    "        self.item_embeddings.weight.data[-1].zero_()\n",
    "        nn.init.uniform_(self.drr_ave.weight)\n",
    "        self.drr_ave.bias.data.zero_()\n",
    "\n",
    "    def forward(self, user, memory):\n",
    "        user = user.to(device)\n",
    "        memory = memory.to(device)\n",
    "        user_embedding = self.user_embeddings(user.long())\n",
    "        \n",
    "\n",
    "        item_embeddings = self.item_embeddings(memory.long())\n",
    "        drr_ave = self.drr_ave(item_embeddings).squeeze(1)\n",
    "        \n",
    "        return torch.cat((user_embedding, user_embedding * drr_ave, drr_ave), 1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:33.943691Z",
     "iopub.status.busy": "2023-04-24T21:02:33.942782Z",
     "iopub.status.idle": "2023-04-24T21:02:33.953507Z",
     "shell.execute_reply": "2023-04-24T21:02:33.952092Z",
     "shell.execute_reply.started": "2023-04-24T21:02:33.943642Z"
    }
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_repr_dim, action_emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_repr_dim + action_emb_dim, hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = self.layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:33.956448Z",
     "iopub.status.busy": "2023-04-24T21:02:33.955543Z",
     "iopub.status.idle": "2023-04-24T21:02:33.988278Z",
     "shell.execute_reply": "2023-04-24T21:02:33.986054Z",
     "shell.execute_reply.started": "2023-04-24T21:02:33.956275Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.initialize()\n",
    "        self.to(device)\n",
    "\n",
    "    def initialize(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight)\n",
    "        \n",
    "    def evaluate(self, action_emb, epsilon=1e-6):\n",
    "                \n",
    "        mean, log_std = torch.chunk(action_emb.float(), 2, dim=-1)\n",
    "        \n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.sample()\n",
    "\n",
    "        act = torch.tanh(z)\n",
    "        \n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - act.pow(2) + epsilon)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "\n",
    "        return action_emb, log_prob, z, mean, log_std\n",
    "    \n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.layers(state)\n",
    "    \n",
    "    def get_action(self, user, memory, state_repr, \n",
    "                   action_emb,\n",
    "                   items=torch.tensor([i for i in range(item_num)]).to(device),\n",
    "                   return_scores=False\n",
    "                  ):\n",
    "        state = state_repr(user, memory).to(device)\n",
    "        items = items.to(device)\n",
    "        scores = torch.bmm(state_repr.item_embeddings(items).unsqueeze(0), \n",
    "                         action_emb.T.unsqueeze(0)).squeeze(0).to(device)\n",
    "        if return_scores:\n",
    "            return scores, torch.gather(items, 0, scores.argmax(0)).to(device)\n",
    "        else:\n",
    "            return torch.gather(items, 0, scores.argmax(0)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:02:33.991161Z",
     "iopub.status.busy": "2023-04-24T21:02:33.990738Z",
     "iopub.status.idle": "2023-04-24T21:03:28.595034Z",
     "shell.execute_reply": "2023-04-24T21:03:28.593646Z",
     "shell.execute_reply.started": "2023-04-24T21:02:33.991123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting dataset\n",
      "Resetting dataset\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = EvalDataset(\n",
    "    np.array(test_data)[np.array(test_data)[:, 0] == 6039], \n",
    "    item_num, \n",
    "    test_matrix)\n",
    "valid_loader = td.DataLoader(valid_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "full_dataset = EvalDataset(np.array(test_data), item_num, test_matrix)\n",
    "full_loader = td.DataLoader(full_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:03:28.597400Z",
     "iopub.status.busy": "2023-04-24T21:03:28.596935Z",
     "iopub.status.idle": "2023-04-24T21:03:28.621044Z",
     "shell.execute_reply": "2023-04-24T21:03:28.619838Z",
     "shell.execute_reply.started": "2023-04-24T21:03:28.597352Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_evaluation(net, state_representation, training_env_memory, loader=valid_loader):\n",
    "    hits = []\n",
    "    dcgs = []\n",
    "    test_env = Env(test_matrix)\n",
    "    test_env.memory = training_env_memory.copy()\n",
    "    user, memory = test_env.reset(int(to_np(next(iter(valid_loader))['user'])[0]))\n",
    "    user.to(device)\n",
    "    memory.to(device)\n",
    "    for batch in loader:\n",
    "        action_emb = net(state_repr(user, memory)).to(device)\n",
    "        scores, action = net.get_action(\n",
    "            batch['user'], \n",
    "            torch.tensor(test_env.memory[to_np(batch['user']).astype(int), :]).to(device), \n",
    "            state_representation, \n",
    "            action_emb,\n",
    "            batch['item'].long(), \n",
    "            return_scores=True\n",
    "        )\n",
    "        user, memory, reward, done = test_env.step(action)\n",
    "\n",
    "        \n",
    "        _, ind = scores[:, 0].topk(10)\n",
    "        predictions = torch.take(batch['item'].to(device), ind).cpu().numpy().tolist()\n",
    "        actual = batch['item'][0].item()\n",
    "        hits.append(hit_metric(predictions, actual))\n",
    "        dcgs.append(dcg_metric(predictions, actual))\n",
    "        \n",
    "    return np.mean(hits), np.mean(dcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:03:28.623523Z",
     "iopub.status.busy": "2023-04-24T21:03:28.622853Z",
     "iopub.status.idle": "2023-04-24T21:03:28.699954Z",
     "shell.execute_reply": "2023-04-24T21:03:28.698736Z",
     "shell.execute_reply.started": "2023-04-24T21:03:28.623488Z"
    }
   },
   "outputs": [],
   "source": [
    "train_env = Env(train_matrix)\n",
    "\n",
    "\n",
    "state_repr = State_Repr_Module(user_num, item_num, params['embedding_dim'] , params['hidden_dim']).to(device)\n",
    "value_net  = ValueNetwork(params['embedding_dim'] * 3, params['embedding_dim'], params['hidden_dim']).to(device)\n",
    "target_value_net = ValueNetwork(params['embedding_dim'] * 3, params['embedding_dim'], params['hidden_dim']).to(device)\n",
    "\n",
    "\n",
    "policy_net = PolicyNetwork(params['embedding_dim'], params['hidden_dim']).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "\n",
    "value_criterion  = nn.MSELoss()\n",
    "soft_q_criterion = nn.MSELoss()\n",
    "\n",
    "value_lr  = 3e-4\n",
    "soft_q_lr = 3e-4\n",
    "policy_lr = 3e-4\n",
    "\n",
    "# value_optimizer  = optim.Adam(value_net.parameters(), lr=value_lr)\n",
    "# policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "\n",
    "replay_buffer_size = params['buffer_size']\n",
    "replay_buffer = Prioritized_Buffer(replay_buffer_size)\n",
    "writer = SummaryWriter(log_dir=params['log_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:03:28.701761Z",
     "iopub.status.busy": "2023-04-24T21:03:28.701424Z",
     "iopub.status.idle": "2023-04-24T21:03:28.709937Z",
     "shell.execute_reply": "2023-04-24T21:03:28.708484Z",
     "shell.execute_reply.started": "2023-04-24T21:03:28.701726Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_value  = Ranger(value_net.parameters(),  lr=params['value_lr'], \n",
    "                          weight_decay=params['value_decay'])\n",
    "optimizer_policy = Ranger(policy_net.parameters(), lr=params['policy_lr'], \n",
    "                          weight_decay=params['policy_decay'])\n",
    "state_repr_optimizer = Ranger(state_repr.parameters(), lr=params['state_repr_lr'], \n",
    "                              weight_decay=params['state_repr_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:03:28.712418Z",
     "iopub.status.busy": "2023-04-24T21:03:28.711914Z",
     "iopub.status.idle": "2023-04-24T21:03:28.731870Z",
     "shell.execute_reply": "2023-04-24T21:03:28.730408Z",
     "shell.execute_reply.started": "2023-04-24T21:03:28.712383Z"
    }
   },
   "outputs": [],
   "source": [
    "def ppo_update(train_env, policy_net, value_net, state_repr, optimizer_policy, optimizer_value, replay_buffer, clip_param=0.1, ppo_epoch=4, mini_batch_size=32, value_coef=0.5, entropy_coef=0.01):\n",
    "    # Sample data from the replay buffer\n",
    "    user, memory, action, reward, next_user, next_memory, done = replay_buffer.sample(mini_batch_size)\n",
    "\n",
    "    # Convert data to tensors and move to the device\n",
    "    user = torch.tensor(user, dtype=torch.long).to(device)\n",
    "    memory = torch.tensor(memory, dtype=torch.float32).to(device)\n",
    "    action = torch.tensor(action, dtype=torch.float32).to(device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "    next_user = torch.tensor(next_user, dtype=torch.long).to(device)\n",
    "    next_memory = torch.tensor(next_memory, dtype=torch.float32).to(device)\n",
    "    done = torch.tensor(done, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Compute the current state and value predictions for the sampled data\n",
    "    state = state_repr(user, memory)\n",
    "    value = value_net(state, policy_net(state))\n",
    "\n",
    "    # Compute the next state and value predictions for the sampled data\n",
    "    with torch.no_grad():\n",
    "        next_state = state_repr(next_user, next_memory)\n",
    "        next_value = value_net(next_state, policy_net(next_state))\n",
    "        mask = 1 - done\n",
    "\n",
    "        # Compute the target value\n",
    "        target_value = reward + mask * next_value\n",
    "\n",
    "    # Compute the advantage\n",
    "    advantage = target_value - value\n",
    "\n",
    "    # Create mini-batches for training\n",
    "    batch_size = user.shape[0]\n",
    "    num_samples = user.shape[0]\n",
    "    indices = torch.randperm(num_samples)\n",
    "    mini_batch_count = num_samples // mini_batch_size\n",
    "\n",
    "    for batch in range(mini_batch_count):\n",
    "        # Compute the actions and log probabilities for the sampled data\n",
    "        batch_indices = indices[batch * mini_batch_size : (batch + 1) * mini_batch_size]\n",
    "        sampled_state = state[batch_indices]\n",
    "        sampled_memory = memory[batch_indices]\n",
    "        sampled_action = action[batch_indices]\n",
    "        sampled_old_log_prob, _, _, _, _ = policy_net.evaluate(sampled_action)\n",
    "\n",
    "        new_log_prob, _, _, _, _ = policy_net.evaluate(sampled_action)\n",
    "        ratio = (new_log_prob - sampled_old_log_prob).exp()\n",
    "\n",
    "        # Compute the clipped surrogate loss\n",
    "        policy_loss1 = ratio * advantage[indices]\n",
    "        policy_loss2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * advantage[indices]\n",
    "        policy_loss = -torch.min(policy_loss1, policy_loss2).mean()\n",
    "\n",
    "        # Compute the value loss\n",
    "        sampled_target_value = target_value[indices]\n",
    "        sampled_value = value[indices]\n",
    "        clipped_value = sampled_value + torch.clamp(sampled_target_value - sampled_value, -clip_param, clip_param)\n",
    "        value_loss1 = (sampled_value - sampled_target_value).pow(2)\n",
    "        value_loss2 = (clipped_value - sampled_target_value).pow(2)\n",
    "        value_loss = 0.5 * torch.max(value_loss1, value_loss2).mean()\n",
    "\n",
    "        # Compute the entropy loss\n",
    "        entropy_loss = -(new_log_prob.exp() * new_log_prob).mean()\n",
    "\n",
    "        # Compute the total loss\n",
    "        loss = policy_loss + value_coef * value_loss - entropy_coef * entropy_loss\n",
    "        \n",
    "        \n",
    "        state_repr_optimizer.zero_grad()\n",
    "        \n",
    "        # Update the policy network\n",
    "        optimizer_policy.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer_policy.step()\n",
    "\n",
    "        # Update the value network\n",
    "        optimizer_value.zero_grad()\n",
    "        value_loss.backward(retain_graph=True)\n",
    "        optimizer_value.step()\n",
    "\n",
    "        state_repr_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-24T21:05:30.486486Z",
     "iopub.status.busy": "2023-04-24T21:05:30.486056Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4699 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  2%|▏         | 114/4699 [04:23<57:27:22, 45.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.04381630386300627, DCG 0.017724674975541557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 225/4699 [06:20<2:25:18,  1.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.019258183853560838, DCG 0.007700193490892519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 324/4699 [11:26<1:30:09,  1.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.011334516927436353, DCG 0.004658194546915398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 438/4699 [16:09<1:40:49,  1.42s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.008500887695577265, DCG 0.0035258384099512343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 551/4699 [23:38<52:27:58, 45.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.006611801541004539, DCG 0.0028333169801887394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 652/4699 [25:53<1:02:33,  1.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.005571055002142713, DCG 0.002409691092017178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 773/4699 [30:51<58:09,  1.12it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.004958851155753404, DCG 0.0021033038548802204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 894/4699 [37:01<45:38,  1.39it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.004425359232471292, DCG 0.0018777882125966604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 1001/4699 [41:46<54:35,  1.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.003909358847657446, DCG 0.0016537260794509392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 1107/4699 [46:38<1:12:36,  1.21s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.003655731539867589, DCG 0.0015154499484454317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1193/4699 [55:28<45:21:54, 46.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.0032971550012681366, DCG 0.0013594984497872543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1282/4699 [58:14<1:18:25,  1.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.0028773580780297532, DCG 0.0011926457052620916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1381/4699 [1:03:26<1:11:44,  1.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.0025887476933033646, DCG 0.001098686101630114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 1467/4699 [1:09:09<59:45,  1.11s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.0024400696163231037, DCG 0.0010357510293893075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1571/4699 [1:16:54<40:30:14, 46.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.0024400696163231037, DCG 0.0010291329393013538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1696/4699 [1:19:33<38:44,  1.29it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.0023263746162793747, DCG 0.0009813367773389063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1792/4699 [1:24:56<44:04,  1.10it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.002265154231640444, DCG 0.0009492803091702799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 1911/4699 [1:30:13<1:21:21,  1.75s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.002221425385469779, DCG 0.0009201580974813043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 2010/4699 [1:35:49<3:24:55,  4.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.00210773038542605, DCG 0.000880694578888594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 2121/4699 [1:41:30<2:17:25,  3.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.0020640015392553853, DCG 0.0008609345359482524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 2227/4699 [1:50:56<38:57:31, 56.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.002142713462362582, DCG 0.0008738356589754199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2358/4699 [1:54:09<20:49,  1.87it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.002160205000830848, DCG 0.0008698665600078085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 2459/4699 [2:00:22<56:07,  1.50s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.002247662693172178, DCG 0.0008870459323876657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 2560/4699 [2:06:25<1:54:44,  3.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.0023701034624500398, DCG 0.0009179746489783227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 2679/4699 [2:12:01<46:48,  1.39s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.0026412223087081623, DCG 0.0009813594386858283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 2790/4699 [2:17:53<53:34,  1.68s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.0030697650011806787, DCG 0.0011004351580624688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 2899/4699 [2:23:45<51:41,  1.72s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.0038481384630185147, DCG 0.0013296797135272174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 3015/4699 [2:29:43<35:26,  1.26s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.004434105001705425, DCG 0.0015089836129924528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 3111/4699 [2:36:28<27:22,  1.03s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.005256207309713926, DCG 0.0017466282875363947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3206/4699 [2:42:24<1:06:08,  2.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.006113292694658959, DCG 0.002001679685851996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3293/4699 [2:48:53<41:31,  1.77s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.01610096116003883, DCG 0.005095897644907555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 3409/4699 [2:55:01<36:32,  1.70s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.019126997315048844, DCG 0.0060385471200033805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3530/4699 [3:01:22<16:56,  1.15it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.02312381385504762, DCG 0.007315381305519839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 3629/4699 [3:07:39<37:00,  2.07s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.02690198616419307, DCG 0.008520789553227895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 3740/4699 [3:14:38<24:30,  1.53s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.03072388731950919, DCG 0.009758634953284766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 3846/4699 [3:21:16<46:21,  3.26s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.03446707655171811, DCG 0.01100225402278662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 3957/4699 [3:27:39<20:10,  1.63s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.038009113091541966, DCG 0.012187708590194723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 4068/4699 [3:34:10<42:43,  4.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit 0.04016057232313868, DCG 0.012946955785816845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 4109/4699 [3:38:22<09:52,  1.00s/it]  "
     ]
    }
   ],
   "source": [
    "train_env = Env(train_matrix)\n",
    "hits, dcgs = [], []\n",
    "hits_all, dcgs_all = [], []\n",
    "step, best_step, best_step_all = 0, 0, 0\n",
    "users = np.random.permutation(appropriate_users)\n",
    "ou_noise = OUNoise(params['embedding_dim'], decay_period=10)\n",
    "\n",
    "for u in tqdm.tqdm(users):\n",
    "    user, memory = train_env.reset(u)\n",
    "    if params['ou_noise']:\n",
    "        ou_noise.reset()\n",
    "    for t in range(int(train_matrix[u].sum())):\n",
    "        state = state_repr(user, memory)\n",
    "        action_emb_old = policy_net(state)\n",
    "        if params['ou_noise']:\n",
    "            action_emb_old = ou_noise.get_action(action_emb_old.detach().cpu().numpy()[0], t)\n",
    "        action_old = policy_net.get_action(\n",
    "            user,\n",
    "            torch.tensor(train_env.memory[to_np(user).astype(int), :]),\n",
    "            state_repr,\n",
    "            action_emb_old,\n",
    "            torch.tensor(\n",
    "                [item for item in train_env.available_items\n",
    "                if item not in train_env.viewed_items]\n",
    "            ).long()\n",
    "        )\n",
    "        user, memory, reward, done = train_env.step(\n",
    "            action_old,\n",
    "            action_emb_old,\n",
    "            buffer=replay_buffer\n",
    "        )\n",
    "\n",
    "        if len(replay_buffer) > params['batch_size']:\n",
    "            ppo_update(train_env, policy_net, value_net, state_repr, policy_optimizer, value_optimizer, replay_buffer)\n",
    "\n",
    "        if step % 100 == 0 and step > 0:\n",
    "            hit, dcg = run_evaluation(policy_net, state_repr, train_env.memory)\n",
    "            writer.add_scalar('hit', hit, step)\n",
    "            writer.add_scalar('dcg', dcg, step)\n",
    "            hits.append(hit)\n",
    "            dcgs.append(dcg)\n",
    "#             print(f'Hit rate: {hit}, dcg: {dcg}')\n",
    "            if np.mean(np.array([hit, dcg]) - np.array([hits[best_step], dcgs[best_step]])) > 0:\n",
    "                best_step = step // 100\n",
    "                torch.save(policy_net.state_dict(), params['log_dir'] + 'policy_net.pth')\n",
    "                torch.save(value_net.state_dict(), params['log_dir'] + 'value_net.pth')\n",
    "                torch.save(state_repr.state_dict(), params['log_dir'] + 'state_repr.pth')\n",
    "        if step % 10000 == 0 and step > 0:\n",
    "            hit, dcg = run_evaluation(policy_net, state_repr, train_env.memory, full_loader)\n",
    "            print(f'Hit {hit}, DCG {dcg}')\n",
    "            writer.add_scalar('hit_all', hit, step)\n",
    "            writer.add_scalar('dcg_all', dcg, step)\n",
    "            hits_all.append(hit)\n",
    "            dcgs_all.append(dcg)\n",
    "            if np.mean(np.array([hit, dcg]) - np.array([hits_all[best_step_all], dcgs_all[best_step_all]])) > 0:\n",
    "                best_step_all = step // 10000\n",
    "                torch.save(policy_net.state_dict(), params['log_dir'] + 'best_policy_net.pth')\n",
    "                torch.save(value_net.state_dict(), params['log_dir'] + 'best_value_net.pth')\n",
    "                torch.save(state_repr.state_dict(), params['log_dir'] + 'best_state_repr.pth')\n",
    "        step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(steps, dcgs, label='nDCG')\n",
    "plt.plot(steps, hits, label='Hit rates')\n",
    "\n",
    "\n",
    "plt.xlabel('Step')\n",
    "plt.title('Hit Rates and nDCG in Each Step')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(hits_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(dcgs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), params['log_dir'] + 'policy_net_final.pth')\n",
    "torch.save(value_net.state_dict(), params['log_dir'] + 'value_net_final.pth')\n",
    "torch.save(state_repr_sac.state_dict(), params['log_dir'] + 'state_repr_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need memory for validation, so it's better to save it and not wait next time \n",
    "with open('logs/memory.pickle', 'wb') as f:\n",
    "    pickle.dump(train_env.memory, f)\n",
    "    \n",
    "with open('logs/memory.pickle', 'rb') as f:\n",
    "    memory = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ou_state_repr = State_Repr_Module(user_num, item_num, params['embedding_dim'], params['hidden_dim'])\n",
    "no_ou_policy_net = PolicyNetwork(params['embedding_dim'], params['hidden_dim'])\n",
    "no_ou_state_repr.load_state_dict(torch.load('logs/final/' + 'best_state_repr.pth'))\n",
    "no_ou_policy_net.load_state_dict(torch.load('logs/final/' + 'best_policy_net.pth'))\n",
    "    \n",
    "hit, dcg = run_evaluation(no_ou_policy_net, no_ou_state_repr, memory, full_loader)\n",
    "print('hit rate: ', hit, 'dcg: ', dcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
